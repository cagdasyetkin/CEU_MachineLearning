---
title: "Machine Learning 1"
subtitle: "Assignment 1 - CEU 2018"
author: "Cagdas Yetkin, Business Analytics Part Time"
date: '2018-01-28'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

## Packages
```{r, message=FALSE, warning=FALSE}
library(doParallel)
library(caret)
library(data.table)
library(purrr)
library(GGally)
library(dplyr)
library(descr)
library(imputeR)
library(tidyr)

registerDoParallel(cores = 4)
theme_set(theme_bw())
```
<h1>1. Model selection with a validation set</h1>

Take the real estate dataset used in class and make log_price your target variable

```{r, messages=FALSE}
data <- fread("kc_house_data.csv")

data[, `:=`(floors = as.numeric(floors), zipcode = factor(zipcode))]
data[, log_price := log(price)]

data[, c("id", "date", "sqft_living15", "sqft_lot15", "price") := NULL]

set.seed(1234)
```
cut your data into three parts: 50% should be your training data, 25% each your validation and test sets (cut data into two parts, then further cut one part into two)
```{r messages=FALSE}
my_ratio <- 0.5
train_indices <- createDataPartition(y = data[["log_price"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train     <- data[train_indices, ]
data_remaining <- data[-train_indices, ]

val_test_indices <- createDataPartition(y = data_remaining[["log_price"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_validation <- data_remaining[val_test_indices, ]
data_test       <- data_remaining[-val_test_indices, ]


```


Train three models on the training data via caret, without cross validation (method = "none"):
<li>a linear model lm with only using sqft_living as a predictor (a simple benchmark)
<li>a linear model lm using all available features
<li>a regression tree (rpart) with cp = 0.0001 (the tune grid should be a dataframe with one column cp and one row with value 0.0001)

For lm models, the tuneGrid argument should not be specified.

```{r messages=FALSE, warning=FALSE}
train_control <- trainControl(method = "none")
simple_linear_fit <- train(log_price ~ sqft_living, 
                  data = data_train, 
                  method = "lm",
                  trControl = train_control)

linear_fit <- train(log_price ~ ., 
                  data = data_train, 
                  method = "lm",
                  trControl = train_control)

tune_grid <- data.frame("cp" = c(0.0001))
rpart_fit <- train(log_price ~ ., 
                   data = data_train, 
                   method = "rpart", 
                   trControl = train_control,
                   tuneGrid = tune_grid)  
```
Compare your models on the validation set and choose the one with the best performance (using RMSE). Use predict.train for prediction just like we used predict in class.

```{r messages=FALSE, warning=FALSE}
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))


simple_linear_rmse <- RMSE(predict.train(simple_linear_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)

simple_linear_rmse
linear_rmse <- RMSE(predict.train(linear_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)
linear_rmse

rpart_rmse <- RMSE(predict.train(rpart_fit, newdata = data_validation, type = 'raw'), data_validation$log_price)
rpart_rmse

```

Evaluate the final model on the test set. Why is it important to have this final set of observations set aside for evaluation?  (Hint: think about what we used the validation set for.)

```{r messages=FALSE, warning=FALSE}
final_performance_measure <- RMSE(predict.train(linear_fit, newdata = data_test, type = 'raw'), data_test$log_price)
final_performance_measure
```

We create partitions for different purposes. We first fit out models on training set. We choose the best performer based on validation set results. Finally we test the winner on observations which our model have never seen before to estimate how it can generalize.

This is like a swimming competition. There was a winner. But we want to see her performance in a different pool condition she has never seen before (or without her husband watching her)

<h1>2. Predicting developer salaries</h1>
Take the real estate dataset used in class and make log_price your target variable.

```{r messages=FALSE}
data <- fread("survey_results_public_selected.csv")

data <- data[!is.na(Salary) & Salary > 0]
data <- data[complete.cases(data)]
data <- data[, Gender := ifelse(Gender == "Male", "Male",
                              ifelse(Gender == "Female", "Female", "Other"))]
large_countries <- data[, .N, by = "Country"][N > 60][["Country"]]
data <- data[, Country := ifelse(Country %in% large_countries, Country, "Other")]
```

<ol style = "list-style-type: lower-alpha">
<li>Describe what the data cleansing steps mean: 1) Keep the observations where the Salary is not NA and greater than zero. 2) Apply complete_cases on top of it to have only the cases without missing values. However, we will need to  fix the missing values in Gender column separetely. They are empty strings. 3) Update the gender column to write 'Other' for the observations where they are not Male or Female. 4) Create a vector for the large countries which have more than 60 observations in data 5) Update the country column in data by using this vector to have country name if it is a large country, call it 'Other' if not.</li>

<li>Using graphs, find at least two interesting features that can contribute to understanding developer salaries</li>

Look at the outcome variable, Salary first...
```{r}
ggplot(data, aes(Salary)) + geom_density()
```



We might want to see the log Salary since we see a right skewed output. Bimodal shape is also creeping into my brain. I will try to discover what is causing this bimodal shape...

```{r}
data[, log_salary := log(Salary)]

ggplot(data, aes(log_salary)) + geom_density()
```



In fact we don't see a nice log-normal but it will not stop us from trying out some models with log Salaries.

What are my variables?
```{r}
names(data)
```
```{r}
data[, .N, by = TabsSpaces][order(N, decreasing = T)]
```
```{r}
ggplot(data, aes(Salary)) + 
    geom_histogram(fill = "tan3", color= "tan4") + 
    facet_wrap(~TabsSpaces) + scale_x_continuous(breaks = 50000)
```


Spacing and Tabbing are referring to different programming languages. These are essentially the differences between low and high level languages. Such as Python (Tab) and C++ (Space). We can see that high level languages such as Python have negative impact on average salaries in our data.


```{r}
data[, .N, by = YearsProgram][order(N, decreasing = T)]
```
```{r}
ggplot(data, aes(Salary)) + 
    geom_histogram(fill = "tan3", color= "tan4") + 
    facet_wrap(~YearsProgram) + scale_x_continuous(breaks = 50000)
```


Here we can see the effect of being an experienced developer! The inexperienced and/or less experienced developers are pulling the salaries to the left significantly.


```{r}
data[, .N, by = Country][order(N, decreasing = T)]
```

```{r messages=FALSE}
ggplot(data, aes(Salary)) + 
    geom_histogram(fill = "tan3", color= "tan4") + 
    facet_wrap(~Country) + scale_x_continuous(breaks = 50000)

```


A quick look reveals that Salaries in India and Others (small countries) have effect on that bimodal shape in the Salary density plot we have seen at the beginning. Country really matters!


```{r}
data[, .N, by = Gender][order(N, decreasing = T)]
```
Gender gap is huge study on its own in IT business. I will not enter into this one at this time.

I want to look at a few more tablular frequencies without calling ggplot.
```{r}
data[, .N, by = FormalEducation][order(N, decreasing = T)]
```

```{r}
data[, .N, by = ProgramHobby][order(N, decreasing = T)]
```



```{r}
data[, .N, by = CompanySize][order(N, decreasing = T)]
```
I have 11 observations missing for Company Size and YearsProgram. 

I have also 464 missing for TabsSpaces. Are they missing at random is the question. It looks like they are similar to the distributions of the entire data. 

The people who didn't give TabsSpace information also didn't give their Gender information (check the table called NoInfoTabSpaces below). Looks like these people don't want to share much info about themselves. 

```{r}
NoInfoTabSpaces <- data[TabsSpaces == '']
ggplot(NoInfoTabSpaces, aes(log_salary)) + geom_density()
NoInfoTabSpaces[, .N, by = Country][order(N, decreasing = T)]
NoInfoTabSpaces[, .N, by = Gender]
NoInfoTabSpaces[, .N, by = YearsProgram][order(N, decreasing = T)]
```

<li>Create a training and a test set assigning 70% to the training set and 30% as the test set.</li>

```{r}
set.seed(123)
my_ratio <- 0.7
train_indices <- createDataPartition(y = data[["log_salary"]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train <- data[train_indices, ]
data_test  <- data[-train_indices, ]

```
<li>Using caret train at least two predictive models to predict the logarithm of Salary (they can be of the same family but with different hyperparameters or they can be of different families like we used lm and rpart in the first exercise). Make sure NOT to include Salary as a predictor variable. Also, just before calling train, remember to use set.seed</li>

```{r}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)

glm_tune_grid <- expand.grid("alpha" = 1,
                             "lambda" = c(0.1, 0.01, 0.001, 0.0001))


lasso_fit <- train(log_salary ~ . -Salary, 
                   data = data_train, 
                   method = "glmnet", 
                   preProcess = c("center", "scale"),
                   tuneGrid = glm_tune_grid,
                   trControl = train_control)

rpart_tune_grid <- data.frame("cp" = c(0.01, 0.001, 0.0001, 0.00001))
rpart_fit <- train(log_salary ~ . -Salary, 
                   data = data_train, 
                   method = "rpart", 
                   preProcess=c("center", "scale"),
                   trControl = train_control,
                   tuneGrid = rpart_tune_grid) 
lasso_fit
rpart_fit
```

### Model selection via cross validation

```{r}
resamps <- resamples(list("regression tree" = rpart_fit,
                          "lasso" = lasso_fit))
summary(resamps)
```

Glmnet Model RMSE is a bit better than Tree based on their RMSE and Rsquared results.
I picked an alpha value of 1, which means LASSO the winner.

<li>evaluate its performance on the test set</li>

```{r}
postResample(data_test$log_salary, predict(lasso_fit, data_test))

test_prediction <- predict.train(lasso_fit, newdata = data_test)
RMSE(test_prediction, data_test[["log_salary"]])
```

<li>Compare the true and predicted values of the test set on a graph. How do you evaluate the model fit based on this graph?</li>

```{r}
data_test[, pred := test_prediction]
ggplot(data_test, aes(log_salary, pred)) + geom_point()
```



Looks like we are making big mistakes
```{r}
data_test[, pred_level := exp(test_prediction)*exp(var(test_prediction)/2)]
```
```{r}
ggplot(data_test, aes(Salary, pred_level)) + geom_point() + geom_smooth(method = 'lm', se = T)
```



Bad news for homoskedasticity!
```{r}
data_test[, errors := Salary - pred_level]
ggplot(data_test, aes(Salary, errors)) + geom_point() + geom_abline(intercept = 0, slope = 0)
```




Errors would cluster around zero randomly if we had a nice model. We are performing a bit better for the low salary groups. However as the real Salary increaes, we are doing more and more mistakes.

We should go back and rethink our strategy. It would be interesting to check how some other models are performing. Especially a random forest with preProcessing for factors to use knnImpute or some more data cleaning and preparation. Because I want to capture the differences based on factors like countries better.



</ol>


<h1>3. Leave-one-out cross validation</h1>

<ol style = "list-style-type: lower-alpha">

<li>
This can be a very expensive computation for large dataset and this is a disadvantage
</li>

<li>
We are maximizing our validation. This is the maximum number of validations we can do with the data on hand.

In k-fold cross validation the prediction error, is defined as the average of the prediction errors obtained on each fold. And there is an overlap in training sets. This overlap is largest on LOOCV (learned models are a bit correlated). This reflects to variance and variance increases. Bias is lower in return with the expense of higher variance. 


</li>

```{r message=FALSE}
library(titanic)
library(data.table)
library(dplyr)
library(caret)

data_train <- data.table(titanic_train)
# recode Survived to factor - needed for binary prediction
data_train[, Survived := factor(ifelse(Survived == 1, "survived", "died"))]
data_train[, Sex := factor(Sex)] #Convert sex to factor
sapply(data_train, function(x) {sum(is.na(x))}) #check for NAs

#do some simple imputation for Age
data_train[, Age := ifelse(is.na(Age), median(Age, na.rm = T), Age), by = Fare]
data_train[, Age := ifelse(is.na(Age), median(Age, na.rm = T), Age)]
#have a look at your data
str(data_train)
summary(data_train)
glimpse(data_train)
```

```{r message=FALSE, warning=FALSE}
set.seed(123)
trctrl_1 <- trainControl(method = "loocv")

loocv_model <- train(Survived ~ Fare + Sex, 
                 data = data_train, 
                 method = "glm",
                 trControl=trctrl_1)


loocv_model

```
and a 10-fold cross-validation estimation using only Fare and Sex as predictor features
```{r message=FALSE}
set.seed(123)
trctrl_2 <- trainControl(method = "cv", 
                       number = 10)
                       #classProbs = TRUE)

cv10_model <- train(Survived ~ Fare + Sex, 
                 data = data_train, 
                 method = "glm",
                 trControl=trctrl_2)
                 #preProcess=c("center", "scale"))
                 #tuneLength=5)

cv10_model
```

```{r}
summary(cv10_model$resample)
```

```{r}
summary(loocv_model$resample)
```
How large are the means?

In sample accuracy is a bit better for LOOCV (78.23% to 78.12%). Doing so many validations can give some boosting effect to our model. However, the threshold between computation time and the business objectives should be carefully assessed.

How do other quantiles look like? Why are quantiles of the accuracy measures of LOOCV so extreme (either 0 or 1)?

LOOCV quartiles are 1 because we do validation for 1 observation. In k-fold cv we have test set observations. We predict some of them correct and some of them wrong. This gives a ratio for each set. When we have only 1 observation this ratio will only be 1 or 0... 


</ol>
